#!/usr/bin/env python
# ai_assistant.py
"""
å‘½ä»¤è¡Œ AI åŠ©æ‰‹ï¼šæ”¯æŒå¤šè½®å¯¹è¯ã€æµå¼è¾“å‡ºã€ä¸Šä¸‹æ–‡è®°å¿†
"""

import os
import sys
import json

import ollama
import typer
import openai
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from dotenv import load_dotenv

app = typer.Typer()
console = Console()

# ---------------------- åˆå§‹åŒ– ---------------------- #
# load_dotenv()  # è¯»å– .env
# openai.api_key = os.getenv("OPENAI_API_KEY")
# if not openai.api_key:
#     console.print("[red]âŒ  æ²¡æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ï¼[/red]")
#     raise SystemExit(1)

# ä¿å­˜å¯¹è¯å†å²ï¼ˆå¯æŒä¹…åŒ–ï¼‰
CHAT_HISTORY_FILE = Path.home() / "PycharmProjects/ProjectKOTeam/src/cliProject/localCliLLM/.ai_cli_chat.json"


def load_history() -> list:
    if CHAT_HISTORY_FILE.exists():
        with CHAT_HISTORY_FILE.open("r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_history(history: list):
    console.print("ä¿å­˜å¯¹è¯å†å²åˆ°ï¼š" + str(CHAT_HISTORY_FILE))
    with CHAT_HISTORY_FILE.open("w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


history = load_history()


# ---------------------- å·¥å…·å‡½æ•° ---------------------- #
def build_messages(history: list, user_input: str) -> list:
    """
    æŠŠå†å²è®°å½•å’Œæœ¬æ¬¡ç”¨æˆ·è¾“å…¥è½¬æ¢æˆ OpenAI çš„ chat æ ¼å¼
    """
    messages = history.copy()
    messages.append({"role": "user", "content": user_input})
    return messages


def print_response(role: str, content: str):
    """
    ç”¨ Rich æ‰“å°è§’è‰²å¯¹è¯
    """
    panel = Panel(content, title=role.title(), expand=False)
    console.print(panel)


def stream_response_online(messages: list):
    load_dotenv()  # è¯»å– .env
    openai.api_key = os.getenv("OPENAI_API_KEY")
    if not openai.api_key:
        console.print("[red]âŒ  æ²¡æœ‰æ‰¾åˆ° OPENAI_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ï¼[/red]")
        raise SystemExit(1)
    """
    ä½¿ç”¨ OpenAI çš„æµå¼æ¥å£
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # æˆ–è€… gpt-3.5-turbo
            messages=messages,
            temperature=0.7,
            stream=True,
        )
        partial_text = ""
        for chunk in response:
            chunk_text = chunk["choices"][0]["delta"].get("content", "")
            if chunk_text:
                partial_text += chunk_text
                # å®æ—¶åˆ·æ–°æ˜¾ç¤º
                console.print(chunk_text, end="", style="cyan")
        console.print()  # æ¢è¡Œ
        return partial_text
    except openai.error.OpenAIError as e:
        console.print(f"[red]OpenAI é”™è¯¯: {e}[/red]")
        return ""


def stream_response(messages: list):
    """
    ä½¿ç”¨ OpenAI çš„æµå¼æ¥å£
    """
    try:

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # æˆ–è€… gpt-3.5-turbo
            messages=messages,
            temperature=0.7,
            stream=True,
        )
        partial_text = ""
        for chunk in response:
            chunk_text = chunk["choices"][0]["delta"].get("content", "")
            if chunk_text:
                partial_text += chunk_text
                # å®æ—¶åˆ·æ–°æ˜¾ç¤º
                console.print(chunk_text, end="", style="cyan")
        console.print()  # æ¢è¡Œ
        return partial_text
    except openai.error.OpenAIError as e:
        console.print(f"[red]OpenAI é”™è¯¯: {e}[/red]")
        return ""


# ---------------------- CLI å‘½ä»¤ ---------------------- #
@app.command("chat")
def chat():
    """
    å¼€å§‹ä¸ AI çš„å¯¹è¯ï¼ˆæ”¯æŒå¤šè½®ï¼‰
    """
    console.print("[green]ğŸ’¬ å¼€å§‹å¯¹è¯ï¼æŒ‰ Ctrl-C é€€å‡ºã€‚[/green]")
    try:
        while True:
            # ç”¨æˆ·è¾“å…¥
            user_text = Prompt.ask("[bold cyan]ä½ [/bold cyan]")
            if not user_text.strip():
                continue

            console.print("è¾“å…¥å†…å®¹ï¼š" + user_text)

            # ç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨
            # msgs = build_messages(history, user_text)

            # è°ƒç”¨æ¨¡å‹ï¼ˆæµå¼ï¼‰
            # assistant_text = stream_response(msgs)

            assistant_text = ollama_stream(user_text, "gemma3n:e4b")

            # è®°å½•åˆ°å†å²
            history.append({"role": "user", "content": user_text})
            history.append({"role": "assistant", "content": assistant_text})
            save_history(history)

    except KeyboardInterrupt:
        console.print("\n[red]ğŸ”š é€€å‡ºå¯¹è¯ã€‚[/red]")
        sys.exit(0)


def ollama_stream(prompt, target_model):
    # è°ƒç”¨ ollama æµå¼ç”Ÿæˆ
    save_data = {"prompt": prompt, "answer": ""}
    for chunk in ollama.generate(model="gemma3n:e4b" if target_model == "gemma3n:e4b" else target_model, prompt=prompt,
                                 stream=True):
        text = chunk.get("response", "")
        if text:
            save_data["answer"] += text
            # å®æ—¶åˆ·æ–°æ˜¾ç¤º
            console.print(text, end="", style="cyan")
    console.print()  # æ¢è¡Œ

    return save_data


@app.command("history")
def history_cmd():
    """æŸ¥çœ‹æœ€è¿‘çš„å¯¹è¯å†å²"""
    if not history:
        console.print("[yellow]æš‚æ— å†å²è®°å½•ã€‚[/yellow]")
        return
    for idx, turn in enumerate(history):
        role = turn["role"].title()
        content = turn["content"]
        panel = Panel(content, title=f"{role} #{idx + 1}", expand=False)
        console.print(panel)


@app.command("clear_history")
def clear_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    if CHAT_HISTORY_FILE.exists():
        CHAT_HISTORY_FILE.unlink()
    global history
    history = []
    console.print("[green]âœ… å¯¹è¯å†å²å·²æ¸…ç©ºã€‚[/green]")


# ---------------------- å…¥å£ ---------------------- #
if __name__ == "__main__":
    app()

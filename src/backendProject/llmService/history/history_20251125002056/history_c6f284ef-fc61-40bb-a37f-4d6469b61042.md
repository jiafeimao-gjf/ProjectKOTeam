# model: gpt-oss:latest
# prompt: 你是大模型开发专家，可以解答大模型从底层到应用实践的问题。
写一个模型公式转换代码 实现 Safetensors 格式到mlmodel 格式

# answer: 
 thinking:

We need to respond in Chinese presumably. The user: "你是大模型开发专家，可以解答大模型从底层到应用实践的问题。写一个模型公式转换代码 实现 Safetensors 格式到mlmodel 格式" So they want code to convert from Safetensors format to mlmodel format. "Safetensors" refers to the format from HuggingFace, a safe binary format. "mlmodel" format is Apple's Core ML model format. So conversion from a safetensors PyTorch or other model to Core ML format.

We need to produce code that can read a safetensors file, load the weights, maybe convert to ONNX or Core ML? There's existing libraries like coremltools that can convert from PyTorch to Core ML. But safetensors is a file format that can be loaded via safetensors library. If the model is originally a huggingface transformer or diffusions? But we need a general converter. We can propose steps: load weights, reconstruct model architecture, convert to Core ML via coremltools.

We can also use the onnx conversion path: use torch to convert to ONNX, then coremltools to convert to mlmodel.

Alternatively, if the model is just linear or simple, we can manually create a mlmodel file using coremltools.

But the question: "写一个模型公式转换代码 实现 Safetensors 格式到mlmodel 格式". They want code that does this conversion.

We need to assume environment: Python, libraries: safetensors, torch, coremltools. Provide code that loads a safetensors file, builds a model, uses torch to export to ONNX, then coremltools to convert to Core ML.

We can provide a skeleton that is customizable. Use huggingface transformers? Possibly use transformers AutoModel to load weights from safetensors. Provide an example for a simple model.

We should include error handling, explanation.

Also, we need to mention that converting may require a model definition; just weights are not enough.

Hence the code will require the model architecture. We'll show an example for a simple feed-forward network or a transformer model.

Also mention that if the safetensors file only contains weights for a specific model, we need to know its architecture. So provide a template.

We can provide a command-line script that takes input safetensors file and output mlmodel file.

Let's produce code in Python, maybe using click or argparse.

Let's include optional support for onnx conversion.

Also mention that coremltools expects a TorchScript model.

We can load the safetensors file via safetensors.torch.load_file.

If the safetensors file is a .safetensors file with model weights, we need to load into a dict, then instantiate the model architecture and load state dict.

For example:

from safetensors.torch import load_file

state_dict = load_file("model.safetensors")

model = MyModel()
model.load_state_dict(state_dict)

Then we can use torch.jit.trace or torch.jit.script to get TorchScript.

We can then export to ONNX: torch.onnx.export(...)

Then convert to mlmodel: coremltools.converters.onnx.convert(onnx_model)

But coremltools can convert from TorchScript directly: coremltools.converters.pytorch.convert(model)

But older versions needed ONNX.

Let's produce a script that works with coremltools 5+.

We'll provide code that uses coremltools to convert TorchScript.

We'll also mention optional usage of huggingface's AutoModelForSequenceClassification.

Ok, produce final answer.

thinking end 

下面给出一个 **完整可运行的 Python 脚本**，演示如何把 `safetensors` 格式的权重文件转换成 Apple 的 Core ML `mlmodel` 格式。  
脚本的思路：

1. **读取权重** – 使用 `safetensors` 库把 `.safetensors` 文件读成字典。  
2. **重建模型** – 你需要知道权重对应的网络结构（例如 Hugging Face 的 Transformer、一个简单的 MLP，或你自己的网络）。  
3. **加载权重** – 把字典 `state_dict` 载入到模型中。  
4. **TorchScript 化** – `torch.jit.trace` 或 `torch.jit.script` 把模型转成可序列化的 TorchScript。  
5. **Core ML 转换** – 通过 `coremltools` 把 TorchScript 转成 `.mlmodel`。  

> ⚠️ **关键点**  
> * 仅有权重文件并不能完整描述模型；你必须在代码里实现或导入相应的网络结构。  
> * 对于大模型（如 BERT、GPT 等），Core ML 的模型大小可能会很大，而且可能需要 `coremltools >= 5.0` 才能直接转换。  
> * 如果你想把模型先转成 ONNX，再再转成 Core ML，可以把第 5 步改为 `coremltools.converters.onnx.convert(onnx_model)`，但这会多一步转换，速度稍慢。  

下面给出一个 **可直接运行** 的模板，演示如何把 **Hugging‑Face 变压器模型** 的权重从 `.safetensors` 转成 `mlmodel`。如果你有自己的网络，只需要把 `MyModel` 换成你自己的类即可。

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Safetensors → Core ML (.mlmodel) 转换脚本
------------------------------------------
依赖：
  - safetensors >= 0.4.0
  - torch >= 1.13
  - transformers >= 4.30
  - coremltools >= 5.0

使用示例：
  python safetensors_to_mlmodel.py \
    --safetensors_file  /path/to/model.safetensors \
    --output_file       /path/to/model.mlmodel \
    --model_class       transformers.AutoModelForSequenceClassification \
    --model_name_or_path  distilbert-base-uncased \
    --input_names       input_ids,attention_mask \
    --output_name       logits

注：如果你使用的是自己的网络，只需要把 `--model_class` 换成自己的类路径即可，
      并相应调整 `--input_names`、`--output_name`。
"""

import argparse
import os
import sys
import torch
import coremltools as ct
from safetensors.torch import load_file

def parse_args():
    parser = argparse.ArgumentParser(
        description="Convert a .safetensors model to .mlmodel (Core ML) format."
    )
    parser.add_argument(
        "--safetensors_file", type=str, required=True,
        help="Path to the .safetensors file containing the weights."
    )
    parser.add_argument(
        "--output_file", type=str, required=True,
        help="Path where the .mlmodel file will be saved."
    )
    parser.add_argument(
        "--model_class", type=str, required=True,
        help="Full python path to the model class, e.g. transformers.AutoModelForSequenceClassification."
    )
    parser.add_argument(
        "--model_name_or_path", type=str, default=None,
        help="If model_class is a HuggingFace AutoModel, provide the model name or local path."
    )
    parser.add_argument(
        "--input_names", type=str, default="input_ids,attention_mask",
        help="Comma‑separated list of input tensor names (order matters)."
    )
    parser.add_argument(
        "--output_name", type=str, default="logits",
        help="Name of the output tensor."
    )
    parser.add_argument(
        "--batch_size", type=int, default=1,
        help="Batch size used for tracing (default 1)."
    )
    return parser.parse_args()

def import_class(path: str):
    """
    Dynamically import a class from a full dotted path, e.g. 'transformers.AutoModelForSequenceClassification'
    """
    module_path, class_name = path.rsplit(".", 1)
    module = __import__(module_path, fromlist=[class_name])
    return getattr(module, class_name)

def build_and_load_model(model_cls, weights_path, model_name_or_path=None):
    """
    1. Instantiate model (may need a pretrained name if it's an AutoModel).
    2. Load state_dict from .safetensors file.
    3. Load state_dict into the model.
    """
    if "AutoModel" in model_cls.__name__ and model_name_or_path is not None:
        # For HuggingFace AutoModel, use `from_pretrained` with local weights
        model = model_cls.from_pretrained(
            model_name_or_path,
            # we pass `state_dict` as a dict to avoid downloading weights again
            state_dict=load_file(weights_path)
        )
    else:
        # For custom models, just instantiate and load state_dict manually
        model = model_cls()
        state_dict = load_file(weights_path)
        model.load_state_dict(state_dict)

    # Put model in eval mode
    model.eval()
    return model

def trace_torchscript(model, input_names, batch_size):
    """
    Create dummy inputs based on the input_names and trace the model.
    """
    # Determine input shapes: for simplicity, we assume each input is of shape (batch, seq_len)
    # and seq_len=128 for transformers. 用户可以自行修改。
    dummy_inputs = {}
    seq_len = 128
    for name in input_names:
        # For transformers, attention_mask is usually int, input_ids is int64
        if "mask" in name:
            dtype = torch.long
        else:
            dtype = torch.long
        dummy_inputs[name] = torch.zeros((batch_size, seq_len), dtype=dtype, device="cpu")

    # The tracing function expects positional arguments, so we build a tuple
    input_tuple = tuple(dummy_inputs[name] for name in input_names)

    # Trace
    traced = torch.jit.trace(model, input_tuple, check_trace=False)
    return traced

def convert_to_coreml(traced, input_names, output_name, output_file):
    """
    Convert TorchScript to Core ML .mlmodel
    """
    # Build Core ML input spec
    inputs = [
        ct.TensorType(name=name, shape=(None, None), dtype=ct.int64)
        for name in input_names
    ]

    # Convert
    mlmodel = ct.convert(
        traced,
        inputs=inputs,
        classifier_config=ct.ClassifierConfig({output_name: "prob"}),
        # 若需要指定模型输入输出名称：
        minimum_deployment_target=ct.target.iOS15
    )

    # 设置输出名称
    mlmodel.output_description[output_name] = "Model output"

    # 保存
    mlmodel.save(output_file)
    print(f"✅ 已保存 Core ML 模型: {output_file}")

def main():
    args = parse_args()

    if not os.path.isfile(args.safetensors_file):
        print(f"❌ 找不到 .safetensors 文件: {args.safetensors_file}", file=sys.stderr)
        sys.exit(1)

    # 动态导入模型类
    model_cls = import_class(args.model_class)

    # 构建并加载模型
    model = build_and_load_model(model_cls, args.safetensors_file, args.model_name_or_path)

    # 解析输入名字
    input_names = [name.strip() for name in args.input_names.split(",") if name.strip()]

    # 1️⃣ TorchScript 化
    print("⚙️ 正在 TorchScript 化模型…")
    traced = trace_torchscript(model, input_names, args.batch_size)

    # 2️⃣ 转 Core ML
    print("⚙️ 正在转换为 Core ML .mlmodel…")
    convert_to_coreml(
        traced,
        input_names,
        args.output_name,
        args.output_file
    )

if __name__ == "__main__":
    main()
```

---

## 使用示例

```bash
# 假设你已经把 distilbert-base-uncased 的权重保存为 model.safetensors
# 你想把它转成 Core ML 模型
python safetensors_to_mlmodel.py \
  --safetensors_file ./model.safetensors \
  --output_file ./distilbert.mlmodel \
  --model_class transformers.AutoModelForSequenceClassification \
  --model_name_or_path distilbert-base-uncased \
  --input_names input_ids,attention_mask \
  --output_name logits
```

### 说明

| 参数 | 含义 | 备注 |
|------|------|------|
| `--safetensors_file` | `.safetensors` 权重文件路径 | 必填 |
| `--output_file` | 目标 `.mlmodel` 路径 | 必填 |
| `--model_class` | 模型类全路径 | 如 `transformers.AutoModelForSequenceClassification` |
| `--model_name_or_path` | 若使用 AutoModel，需要模型名称或本地路径 | 传入后自动使用 `from_pretrained` 加载权重 |
| `--input_names` | 输入张量名（逗号分隔） | 需与模型 `forward` 参数保持一致 |
| `--output_name` | 输出张量名 | 例如 `logits` |
| `--batch_size` | 用于 tracing 的 batch 大小 | 默认 1，调整可提升 trace 效率 |

---

## 常见问题

| 问题 | 解决方案 |
|------|----------|
| **转换失败：`RuntimeError: Some weights of the model checkpoint were not used`** | 说明 `safetensors` 权重与模型结构不匹配。请确认你提供的模型类与权重文件对应。 |
| **输出大小过大** | Core ML 对大型模型的支持有限；可以尝试做量化（`ct.convert(..., compute_units=ct.ComputeUnit.ALL, prune=True)`）或改为 ONNX 先行压缩。 |
| **Core ML 运行时崩溃** | 检查输入张量的 `dtype` 和 `shape` 与 Core ML 的期望是否一致。 |
| **需要把模型改成 TorchScript 之前进行优化** | 可以在 `trace_torchscript` 里加上 `torch.jit.optimize_for_inference(traced)` 或 `torch.jit.freeze(traced)`。 |

---

> **小结**  
> 1️⃣ 先确认你有完整的模型结构。  
> 2️⃣ 用 `safetensors` 把权重读出来，装进结构里。  
> 3️⃣ 用 `torch.jit.trace` 或 `torch.jit.script` 生成 TorchScript。  
> 4️⃣ 用 `coremltools` 把 TorchScript 转成 `.mlmodel`。  

这样你就可以把任意 `.safetensors` 权重（只要你能给出对应的网络定义）快速转成 Apple 生态可以直接跑的 Core ML 模型了。祝你实验顺利 🚀！thinking:



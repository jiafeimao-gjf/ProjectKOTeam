# prompt: 如何实现命令行交互的ai助手
# answer: 
 ## 1️⃣ 先想清楚你想实现什么

| 目标 | 说明 |
|------|------|
| **交互方式** | 纯命令行（输入、输出），可选择“实时流”或“一次性返回”。 |
| **模型来源** | ① OpenAI（ChatGPT、GPT‑4 等）<br>② 本地开源模型（Llama‑2、Mistral、ChatGLM 等）<br>③ 结合多模型的混合系统 |
| **语言/框架** | Python 3.8+（最常用）<br>推荐：`openai` SDK、`typer`/`click`/`prompt_toolkit`/`rich` |
| **部署环境** | 本地终端、Docker、云服务器、容器化 |


---

## 2️⃣ 环境搭建（以 Python + OpenAI 为例）

```bash
# 1. 创建虚拟环境
python3 -m venv ai-cli
source ai-cli/bin/activate

# 2. 安装必要库
pip install openai
pip install typer
pip install rich
pip install python-dotenv   # 方便管理 .env
```

> **提示**：如果你想用本地模型，改用 `pip install transformers accelerate bitsandbytes` 等即可。

> **API Key**  
> 将 OpenAI 的 key 写进 `.env`（不推送到版本控制）  
> ```
> OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
> ```

---

## 3️⃣ 代码实现（Python + Typer + Rich）

> **目录结构**  
> ```
> ai-cli/
> ├── ai_assistant.py
> └── .env
> ```

```python
#!/usr/bin/env python
# ai_assistant.py
"""
命令行 AI 助手：支持多轮对话、流式输出、上下文记忆
"""

import os
import sys
import json
import time
import typer
import openai
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from dotenv import load_dotenv

app = typer.Typer()
console = Console()

# ---------------------- 初始化 ---------------------- #
load_dotenv()  # 读取 .env
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    console.print("[red]❌  没有找到 OPENAI_API_KEY，请在 .env 中配置！[/red]")
    raise SystemExit(1)

# 保存对话历史（可持久化）
CHAT_HISTORY_FILE = Path.home() / ".ai_cli_chat.json"

def load_history() -> list:
    if CHAT_HISTORY_FILE.exists():
        with CHAT_HISTORY_FILE.open("r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history(history: list):
    with CHAT_HISTORY_FILE.open("w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

history = load_history()

# ---------------------- 工具函数 ---------------------- #
def build_messages(history: list, user_input: str) -> list:
    """
    把历史记录和本次用户输入转换成 OpenAI 的 chat 格式
    """
    messages = history.copy()
    messages.append({"role": "user", "content": user_input})
    return messages

def print_response(role: str, content: str):
    """
    用 Rich 打印角色对话
    """
    panel = Panel(content, title=role.title(), expand=False)
    console.print(panel)

def stream_response(messages: list):
    """
    使用 OpenAI 的流式接口
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",          # 或者 gpt-3.5-turbo
            messages=messages,
            temperature=0.7,
            stream=True,
        )
        partial_text = ""
        for chunk in response:
            chunk_text = chunk["choices"][0]["delta"].get("content", "")
            if chunk_text:
                partial_text += chunk_text
                # 实时刷新显示
                console.print(chunk_text, end="", style="cyan")
        console.print()  # 换行
        return partial_text
    except openai.error.OpenAIError as e:
        console.print(f"[red]OpenAI 错误: {e}[/red]")
        return ""

# ---------------------- CLI 命令 ---------------------- #
@app.command()
def chat():
    """
    开始与 AI 的对话（支持多轮）
    """
    console.print("[green]💬 开始对话！按 Ctrl-C 退出。[/green]")
    try:
        while True:
            # 用户输入
            user_text = Prompt.ask("[bold cyan]你[/bold cyan]")
            if not user_text.strip():
                continue

            # 生成消息列表
            msgs = build_messages(history, user_text)

            # 调用模型（流式）
            assistant_text = stream_response(msgs)

            # 记录到历史
            history.append({"role": "user", "content": user_text})
            history.append({"role": "assistant", "content": assistant_text})
            save_history(history)

    except KeyboardInterrupt:
        console.print("\n[red]🔚 退出对话。[/red]")
        sys.exit(0)

@app.command()
def history_cmd():
    """查看最近的对话历史"""
    if not history:
        console.print("[yellow]暂无历史记录。[/yellow]")
        return
    for idx, turn in enumerate(history):
        role = turn["role"].title()
        content = turn["content"]
        panel = Panel(content, title=f"{role} #{idx+1}", expand=False)
        console.print(panel)

@app.command()
def clear_history():
    """清空对话历史"""
    if CHAT_HISTORY_FILE.exists():
        CHAT_HISTORY_FILE.unlink()
    global history
    history = []
    console.print("[green]✅ 对话历史已清空。[/green]")

# ---------------------- 入口 ---------------------- #
if __name__ == "__main__":
    app()
```

### 运行方式

```bash
# 1. 让脚本可执行
chmod +x ai_assistant.py

# 2. 直接执行
./ai_assistant.py chat

# 3. 查看历史
./ai_assistant.py history

# 4. 清空历史
./ai_assistant.py clear_history
```

> **Tip**：如果想把 `ai_assistant.py` 注册为系统命令，可以放到 `/usr/local/bin` 或者使用 `python -m pip install -e .` 的方式。

---

## 4️⃣ 若想用 **本地模型**（例如 Llama‑2）

```bash
pip install transformers accelerate bitsandbytes
```

然后把 `stream_response` 换成：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    device_map="auto",  # 自动分配 GPU
    torch_dtype="auto"
)
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    temperature=0.7,
    top_p=0.95,
    streaming=True,
)

def local_stream_response(messages: list):
    prompt = ""
    for turn in messages:
        role = turn["role"]
        content = turn["content"]
        prompt += f"{role}: {content}\n"
    prompt += "assistant:"

    partial = ""
    for chunk in pipe(prompt):
        chunk_text = chunk[0]["generated_text"][len(partial):]
        partial += chunk_text
        console.print(chunk_text, end="", style="cyan")
    console.print()
    return partial
```

> **注意**：本地模型需要 GPU、显存以及足够的时间来加载。若使用 CPU，建议改用 7B 版或 3B 版，甚至 1.3B 之类的轻量模型。

---

## 5️⃣ 功能扩展建议

| 功能 | 说明 |
|------|------|
| **系统提示** | 在每次对话前插入 `system` 角色，提供上下文限制或角色设定。 |
| **多语言支持** | 通过 `translate` 工具或直接在 prompt 里指定语言。 |
| **文件或网页阅读** | 读取本地文件、PDF、网页内容，提取关键信息后作为系统提示。 |
| **命令模式** | 用 `typer` 的 `@app.command()` 实现子命令（如 `summarize`, `translate`, `search`）。 |
| **日志与计费** | 记录每次调用的 token 使用量，结合 OpenAI 的计费 API 监控花费。 |
| **缓存** | 对常见问题缓存回答，减少 API 调用。 |
| **Web UI** | 通过 `streamlit`、`gradio` 或 `FastAPI` + `uvicorn` 打造网页前端。 |

---

## 6️⃣ 安全与成本管理

| 关注点 | 解决方案 |
|--------|----------|
| **API Key 泄露** | 用 `.env`、`.gitignore` 或 HashiCorp Vault 管理，永不硬编码。 |
| **超支风险** | 在脚本里加 `max_tokens` 限制；使用 `openai.Usage` API 检查月度花费。 |
| **输入敏感信息** | 对包含 PII 的内容做脱敏或不上传。 |
| **速率限制** | 通过 `time.sleep` 或 `retry` 处理 `429` 状态码。 |

---

## 7️⃣ 进一步阅读

- [OpenAI 官方文档](https://platform.openai.com/docs)
- [Python OpenAI SDK](https://github.com/openai/openai-python)
- [Typer - Fast, typed CLI](https://typer.tiangolo.com/)
- [Rich - beautiful terminal output](https://rich.readthedocs.io/)
- [Transformers - Hugging Face](https://huggingface.co/docs/transformers/index)

---

**小结**  
以上脚本已经包含了：  
1️⃣ 终端交互 + 多轮对话  
2️⃣ 对话历史存档与清空  
3️⃣ 流式输出与即时反馈  
4️⃣ 可快速切换 OpenAI / 本地模型  

你只需根据自己的需求把模型、prompt 或其他插件插进去，就能得到一个功能完整、可持续扩展的命令行 AI 助手。祝你玩得愉快 🚀！

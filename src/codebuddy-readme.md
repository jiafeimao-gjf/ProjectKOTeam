### 项目理解总结

#### 1. **项目概述**
- **目标**：通过大模型能力构建一个自动化的外包团队或软件项目孵化器。
- **功能**：结合大模型辅助规划功能点，利用本地部署模型测试功能。

#### 2. **项目结构**
- **前端项目**：
  - 路径：`/src/frontendProject/aiSEfrontend`
  - 技术栈：基于 Vite 初始化的 Vue 3 项目，UI 由 Copilot 辅助开发。
  - 主要功能：
    - **问答模式**：支持一问一答，选择本地模型进行交互。
    - **软件开发链式回答**：结合预置 Prompt，生成多轮回答，完善项目开发文档和代码输出。
    - **自定义链式回答**：支持自定义 Prompt 和问题，生成多轮回答。
  - 主要组件：
    - `App.vue`：主入口，提供模式切换（问答、软件项目模式、自定义项目模式）。
    - `QnA.vue`：问答模式的核心组件，支持问题输入、模型选择和流式回答展示。
    - `ProjectEngine.vue` 和 `CustomProjectEngine.vue`：分别用于软件项目模式和自定义项目模式。

- **后端项目**：
  - 路径：`/src/backendProject/llmService`
  - 技术栈：基于 Flask 构建，集成 Ollama 模型部署和调用功能。
  - 主要功能：
    - **模型调用**：通过 `/chat_start` 和 `/chat` 接口支持流式问答。
    - **历史记录**：将问答历史保存到本地文件（`history` 目录）。
    - **MongoDB 集成**：支持将问答历史存储到 MongoDB（当前注释状态）。
  - 核心文件：
    - `app.py`：主应用文件，定义接口逻辑和模型调用。

#### 3. **核心流程**
1. **前端**：
   - 用户输入问题并选择模型。
   - 前端通过 `/chat_start` 接口发起请求，获取流式通道地址。
   - 通过 EventSource 接收流式回答并实时渲染。

2. **后端**：
   - 接收问题并调用 Ollama 模型生成回答。
   - 流式返回回答内容，并保存问答历史到本地文件。

#### 4. **技术亮点**
- **流式交互**：前后端通过 SSE（Server-Sent Events）实现流式问答。
- **多模式支持**：支持问答、软件项目开发和自定义项目开发三种模式。
- **本地模型集成**：通过 Ollama 本地部署模型，支持多种模型选择。

#### 5. **后续建议**
- **完善文档**：补充各模块的详细说明和接口文档。
- **扩展功能**：
  - 支持更多模型和自定义 Prompt。
  - 增加用户管理和项目协作功能。
- **优化性能**：优化流式回答的渲染速度和历史记录管理。

如果需要进一步了解某个模块或功能，请告诉我！